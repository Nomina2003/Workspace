{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "853f5dbf",
      "metadata": {},
      "source": [
        "# Теория игр: Практикум в Jupyter Notebook\n",
        "\n",
        "Этот ноутбук предназначен для закрепления материала по **теории игр** на примерах и практических задачах. Мы разберём несколько матричных игр, научимся находить оптимальные стратегии в **чистом** и **смешанном** виде, а также познакомимся с библиотеками Python, которые упрощают расчёты и позволяют проводить небольшие симуляции.\n",
        "\n",
        "## Краткое содержание\n",
        "1. **Подготовка окружения** (библиотеки и функции).\n",
        "2. **Пример 1**: Игра 2×2 с седловой точкой.\n",
        "3. **Пример 2**: Игра 2×2 без седловой точки (поиск смешанных стратегий).\n",
        "4. **Пример 3**: \"Камень-ножницы-бумага\" (3×3). Симуляция.\n",
        "5. (Опционально) **Линейное программирование** для решения матричной игры.\n",
        "7. Дополнительные практические задания с решениями (3 уровня сложности).\n",
        "\n",
        "Каждый раздел содержит краткую теорию, кодовые примеры, а также задания для самостоятельной работы.\n",
        "\n",
        "> **Совет**: чтобы по-настоящему разобраться в материале, обязательно экспериментируйте с кодом, меняйте параметры матриц, пробуйте разные вероятности стратегий и смотрите на результаты."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3d2fd36",
      "metadata": {},
      "source": [
        "## 1. Подготовка окружения\n",
        "\n",
        "В этом разделе:\n",
        "1. Подключим основные библиотеки.\n",
        "2. Определим вспомогательную функцию `expected_payoff`.\n",
        "\n",
        "**Используемые библиотеки**:\n",
        "- **NumPy**: работа с матрицами и векторами.\n",
        "- **Nashpy**: поиск равновесий Нэша в матричных играх.\n",
        "\n",
        "### Функция `expected_payoff`\n",
        "Она считает математическое ожидание выигрыша первого игрока при заданных **смешанных** стратегиях:\n",
        "- `A`: платежная матрица (выигрыш первого игрока);\n",
        "- `P`: вероятности (распределение) первого игрока;\n",
        "- `Q`: вероятности второго игрока.\n",
        "Возвращает число (скаляр) — ожидаемый выигрыш первого игрока."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "27e59e40",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import nashpy as nash\n",
        "\n",
        "def expected_payoff(A, P, Q):\n",
        "    \"\"\"\n",
        "    Возвращает ожидаемый выигрыш первого игрока.\n",
        "    A : np.ndarray (матрица выигрышей первого игрока)\n",
        "    P : список или массив вероятностей для первого игрока\n",
        "    Q : список или массив вероятностей для второго игрока\n",
        "    \"\"\"\n",
        "    # Преобразуем P, Q в numpy-массивы, чтобы удобно делать матричные операции\n",
        "    P = np.array(P)\n",
        "    Q = np.array(Q)\n",
        "    # Математическое ожидание: P * A * Q^T\n",
        "    return P.dot(A).dot(Q.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "472fe516",
      "metadata": {},
      "source": [
        "## 2. Пример 1: Игра 2×2 с седловой точкой\n",
        "\n",
        "**Теория (кратко)**:\n",
        "- Если в матрице $A$ существует элемент, который одновременно:\n",
        "  1) **максимален в своей строке**;\n",
        "  2) **минимален в своём столбце**,\n",
        "  то этот элемент называется **седловой точкой**. Такие пары стратегий в чистом виде дают равновесие (никто из игроков не имеет стимула отклоняться).\n",
        "\n",
        "**Матрица**:\n",
        "$$\n",
        "A = \\begin{pmatrix}\n",
        "3 & 2 \\\\\n",
        "4 & 1\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "Где $A_{ij}$ — выигрыш первого игрока.\n",
        "\n",
        "### 2.1. Код для поиска седловой точки\n",
        "Выполним поиск автоматически, проверяя какие элементы являются макс. в строке и мин. в столбце.\n",
        "\n",
        "> Обратите внимание, что если **такого элемента нет**, игра не имеет равновесия в чистых стратегиях и игрокам придётся использовать **смешанные** стратегии."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "27d05e21",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Матрица A:\n",
            " [[3 2]\n",
            " [4 1]]\n",
            "Максимумы строк: [3 4]\n",
            "Минимумы столбцов: [3 1]\n",
            "Седловая точка (точки): [(0, 0)]\n",
            "Седловая точка = A[0,0] = 3\n"
          ]
        }
      ],
      "source": [
        "# Пример платежной матрицы 2x2\n",
        "A = np.array([\n",
        "    [3, 2],\n",
        "    [4, 1]\n",
        "])\n",
        "\n",
        "# Максимум в каждой строке\n",
        "row_max = A.max(axis=1)\n",
        "# Минимум в каждом столбце\n",
        "col_min = A.min(axis=0)\n",
        "\n",
        "print(\"Матрица A:\\n\", A)\n",
        "print(\"Максимумы строк:\", row_max)\n",
        "print(\"Минимумы столбцов:\", col_min)\n",
        "\n",
        "sedlovye_tochki = []\n",
        "for i in range(A.shape[0]):\n",
        "    for j in range(A.shape[1]):\n",
        "        # Проверяем условие \"макс в строке, мин в столбце\"\n",
        "        if A[i, j] == row_max[i] and A[i, j] == col_min[j]:\n",
        "            sedlovye_tochki.append((i, j))\n",
        "\n",
        "if sedlovye_tochki:\n",
        "    print(\"Седловая точка (точки):\", sedlovye_tochki)\n",
        "    for (i,j) in sedlovye_tochki:\n",
        "        print(f\"Седловая точка = A[{i},{j}] =\", A[i,j])\n",
        "else:\n",
        "    print(\"Седловая точка не найдена.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87dcd0f4",
      "metadata": {},
      "source": [
        "### 2.2. Задание для самостоятельной работы (Задание 1)\n",
        "\n",
        "**Задание**: Измените значения в матрице `A` и проверьте, будет ли седловая точка. Попробуйте:\n",
        "1. `A = [[2, 2],[2, 2]]` (все элементы одинаковые).\n",
        "2. `A = [[0, 4],[1, 3]]`.\n",
        "3. Любую другую 2×2 матрицу, которую захотите.\n",
        "\n",
        "После запуска кода посмотрите, найдена ли седловая точка, и каков её выигрыш.\n",
        "\n",
        "*Подумайте*, что означает ситуация без седловой точки: почему игроки не могут просто выбрать чистые стратегии и быть уверенными в «лучшем» результате?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce98717e",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8b178ea6",
      "metadata": {},
      "source": [
        "## 3. Пример 2: Игра 2×2 без седловой точки\n",
        "\n",
        "**Матрица**:\n",
        "$$\n",
        "A = \\begin{pmatrix}\n",
        "1 & 3 \\\\\n",
        "2 & 0\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "Теория игр подсказывает, что для этой матрицы **седловой точки нет**. Следовательно, игроки должны использовать **смешанные** стратегии, чтобы достичь равновесия.\n",
        "\n",
        "### 3.1. Использование библиотеки Nashpy\n",
        "Создадим объект `Game(A)` и воспользуемся методом `support_enumeration()` для поиска равновесий Нэша. Если равновесие существует, он нам его покажет (с точки зрения распределений $p$ и $q$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "cec1878c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Матрица A:\n",
            " [[1 3]\n",
            " [2 0]]\n",
            "Найдены равновесия (p*, q*):\n",
            "(array([0.5, 0.5]), array([0.75, 0.25]))\n",
            "\n",
            "Оптимальная стратегия первого игрока: [0.5 0.5]\n",
            "Оптимальная стратегия второго игрока: [0.75 0.25]\n",
            "Значение игры (выигрыш первого игрока): 1.5\n"
          ]
        }
      ],
      "source": [
        "A = np.array([\n",
        "    [1, 3],\n",
        "    [2, 0]\n",
        "])\n",
        "print(\"Матрица A:\\n\", A)\n",
        "\n",
        "# Создаём игру на базе матрицы A\n",
        "game_2x2 = nash.Game(A)\n",
        "equilibria = list(game_2x2.support_enumeration())\n",
        "\n",
        "print(\"Найдены равновесия (p*, q*):\")\n",
        "for eq in equilibria:\n",
        "    print(eq)\n",
        "\n",
        "# Обычно вернётся один кортеж (p_star, q_star)\n",
        "p_star, q_star = equilibria[0]\n",
        "print(\"\\nОптимальная стратегия первого игрока:\", p_star)\n",
        "print(\"Оптимальная стратегия второго игрока:\", q_star)\n",
        "\n",
        "# Вычислим значение игры - ожидаемый выигрыш первого игрока\n",
        "V = expected_payoff(A, p_star, q_star)\n",
        "print(\"Значение игры (выигрыш первого игрока):\", V)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3570ab67",
      "metadata": {},
      "source": [
        "### 3.2. Задание для самостоятельной работы (Задание 2)\n",
        "1. **Аналитическое уравнение**. Попробуйте вручную (на бумаге) вывести уравнение безразличия для первого и второго игрока. Получите те же самые $p^*$ и $q^*$. Сравните с результатом `support_enumeration()`.\n",
        "2. **Измените матрицу** `A` на другую 2×2 (где нет седловой точки). Убедитесь, что Nashpy находит другие распределения $p, q$.\n",
        "3. **Попробуйте** оценить `expected_payoff(A, p, q)` для нескольких комбинаций $p, q$ (например, 0.0, 0.25, 0.5, 1.0) и посмотрите, как это влияет на выигрыш.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39f953b3",
      "metadata": {},
      "source": [
        "## 4. Пример 3: \"Камень-ножницы-бумага\" (3×3). Симуляция\n",
        "\n",
        "### 4.1. Постановка задачи\n",
        "Классическая игра: **камень, ножницы, бумага**. Для первого игрока зададим платежную матрицу:\n",
        "$$\n",
        "A = \\begin{pmatrix}\n",
        " 0 & -1 & 1  \\\\\n",
        " 1 &  0 & -1 \\\\\n",
        " -1 & 1 &  0\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "Смысл:\n",
        "- Камень (0) бьёт ножницы (2),\n",
        "- Бумага (1) бьёт камень (0),\n",
        "- Ножницы (2) бьют бумагу (1).\n",
        "\n",
        "В теории хорошо известно, что равновесие здесь — **равномерное** $(1/3, 1/3, 1/3)$. Но мы проверим это с помощью `nashpy` и короткой симуляции."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6c7f00a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Найдены равновесия Нэша (первый игрок, второй игрок):\n",
            "(array([0.33333333, 0.33333333, 0.33333333]), array([0.33333333, 0.33333333, 0.33333333]))\n"
          ]
        }
      ],
      "source": [
        "# Классическая матрица (RPS)\n",
        "A = np.array([\n",
        "    [0, -1,  1],\n",
        "    [1,  0, -1],\n",
        "    [-1, 1,  0]\n",
        "])\n",
        "\n",
        "game_rps = nash.Game(A)\n",
        "eqs_rps = list(game_rps.support_enumeration())\n",
        "print(\"Найдены равновесия Нэша (первый игрок, второй игрок):\")\n",
        "for eq in eqs_rps:\n",
        "    print(eq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb699861",
      "metadata": {},
      "source": [
        "### 4.3. Симуляция случайных партий\n",
        "\n",
        "Чтобы увидеть, как стратегии работают \"в реальном бою\", устроим несколько раундов. Например:\n",
        "- Первый игрок играет строго $(1/3, 1/3, 1/3)$.\n",
        "- Второй игрок предпочитает **камень** на 90%, а ножницы и бумагу лишь по 5%.\n",
        "\n",
        "Посмотрим, какой **средний** выигрыш получится у первого игрока за N=100000 раундов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0b816263",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "При стратегии первого (1/3,1/3,1/3) и второго (0.9,0.05,0.05):\n",
            "Средний выигрыш первого игрока: -0.00153\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def single_round_payoff(A, strategy1, strategy2):\n",
        "    \"\"\"\n",
        "    Одна партия игры: первый игрок выбирает i по strategy1,\n",
        "    второй – j по strategy2. Возвращаем A[i, j].\n",
        "    \"\"\"\n",
        "    # i – индекс строки (стратегия 1-го игрока)\n",
        "    i = np.random.choice(len(strategy1), p=strategy1)\n",
        "    # j – индекс столбца (стратегия 2-го игрока)\n",
        "    j = np.random.choice(len(strategy2), p=strategy2)\n",
        "    return A[i, j]\n",
        "\n",
        "# Матрица RPS та же\n",
        "A_rps = np.array([\n",
        "    [0, -1,  1],\n",
        "    [1,  0, -1],\n",
        "    [-1, 1,  0]\n",
        "])\n",
        "\n",
        "strategy_1 = [1/3, 1/3, 1/3]  # (камень, бумага, ножницы) равномерно\n",
        "strategy_2 = [0.9, 0.05, 0.05] # преимущественно камень\n",
        "\n",
        "N = 100000\n",
        "total_payoff = 0\n",
        "for _ in range(N):\n",
        "    total_payoff += single_round_payoff(A_rps, strategy_1, strategy_2)\n",
        "\n",
        "avg_payoff = total_payoff / N\n",
        "print(\"При стратегии первого (1/3,1/3,1/3) и второго (0.9,0.05,0.05):\")\n",
        "print(\"Средний выигрыш первого игрока:\", avg_payoff)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cd985ce",
      "metadata": {},
      "source": [
        "### 4.4. Задание для самостоятельной работы (Задание 3)\n",
        "1. **Измените** стратегию второго игрока (например, 50% камень, 30% ножницы, 20% бумага) и запустите симуляцию, чтобы узнать средний результат.\n",
        "2. **Попробуйте** заставить обоих игроков играть равномерно (1/3, 1/3, 1/3). Проверьте, что средний выигрыш первого игрока близок к 0 (увеличьте `N`, чтобы уменьшить флуктуации).\n",
        "3. **(Дополнительно)** Попробуйте немного менять стратегию первого игрока (увеличить/уменьшить долю «бумаги»), чтобы увидеть, меняется ли средний выигрыш. Должен ли он по идее становиться больше 0?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5d74be7",
      "metadata": {},
      "source": [
        "## 5. (Опционально) Линейное программирование в SciPy\n",
        "\n",
        "Чтобы показать альтернативный общий метод решения (особенно для \"минимакс\" задач), можно сводить поиск оптимальной стратегии первого игрока к **задаче линейного программирования**. Ниже — пример кода, как это реализовать для 2×2.\n",
        "\n",
        "> **Примечание**: Этот блок можно пропустить, если фокус урока — только на `nashpy` и базовых симуляциях, без погружения в LP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f7d1bdde",
      "metadata": {
        "executionInfo": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Оптимальная стратегия первого игрока: [0.5 0.5]\n",
            "Значение игры: 1.5\n"
          ]
        }
      ],
      "source": [
        "from scipy.optimize import linprog\n",
        "\n",
        "# Пример: A = [[1,3],[2,0]] (из Примера 2)\n",
        "A_lp = np.array([\n",
        "    [1, 3],\n",
        "    [2, 0]\n",
        "])\n",
        "m, n = A_lp.shape\n",
        "\n",
        "# Переменные: p_1, p_2, ..., p_m, v\n",
        "# Цель: макс v <=> min -v\n",
        "# Вектор c: у нас m+1 переменных (p_i и v)\n",
        "c = np.zeros(m + 1)\n",
        "c[-1] = -1  # т.к. минимизируем -v\n",
        "\n",
        "# Ограничение: sum(p_i) = 1\n",
        "A_eq = np.ones((1, m+1))\n",
        "A_eq[0, -1] = 0  # v не участвует в сумме p_i\n",
        "b_eq = np.array([1])\n",
        "\n",
        "# Неравенства: sum_i(p_i*A[i,j]) >= v, для каждого j\n",
        "# Перепишем: - sum_i(p_i*A[i,j]) + v <= 0\n",
        "A_ineq = []\n",
        "b_ineq = []\n",
        "\n",
        "for j in range(n):\n",
        "    row = np.zeros(m + 1)\n",
        "    for i in range(m):\n",
        "        row[i] = -A_lp[i,j]\n",
        "    row[-1] = 1  # коэффициент при v\n",
        "    A_ineq.append(row)\n",
        "    b_ineq.append(0)\n",
        "\n",
        "A_ineq = np.array(A_ineq)\n",
        "b_ineq = np.array(b_ineq)\n",
        "\n",
        "# p_i >= 0, v может быть любым (но фактически ограничивается условиями)\n",
        "bounds = [(0, None)]*m + [(None, None)]\n",
        "\n",
        "res = linprog(c, A_ineq, b_ineq, A_eq, b_eq, bounds=bounds, method='highs')\n",
        "if res.success:\n",
        "    p = res.x[:-1]\n",
        "    v = res.x[-1]\n",
        "    print(\"Оптимальная стратегия первого игрока:\", p / p.sum())\n",
        "    print(\"Значение игры:\", v)\n",
        "else:\n",
        "    print(\"LP не нашёл решения.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "additional-tasks",
      "metadata": {},
      "source": [
        "## 6. Практические задания с решениями\n",
        "\n",
        "Ниже приведён набор заданий **трёх уровней сложности** (лёгкий, средний, сложный). В каждом уровне по три пункта. После формулировки приведены **решения** для самопроверки — **каждое решение находится в отдельной кодовой ячейке**.\n",
        "\n",
        "> **Важно**: сначала постарайтесь выполнить задания самостоятельно, а только потом смотрите решения.\n",
        "\n",
        "---\n",
        "### 6.1. Уровень «Лёгкий»\n",
        "\n",
        "**(A1)** Возьмите матрицу из Примера 1 (2×2) и вручную (или с помощью кода) проверьте, почему в этой игре есть седловая точка.\n",
        "\n",
        "**(A2)** Измените матрицу Примера 1 так, чтобы не было седловой точки. Убедитесь в этом программно. Подсказка: поменяйте 1–2 элемента, чтобы нарушилось условие \"макс в строке = мин в столбце\".\n",
        "\n",
        "**(A3)** Используя функцию `expected_payoff`, посчитайте ожидаемый выигрыш первого игрока в Примере 2 (матрица `[[1,3],[2,0]]`), если первый игрок всегда играет первую строку (100%), а второй — первый столбец (100%).\n",
        "\n",
        "#### Решения (Лёгкий)\n",
        "\n",
        "**(A1) Решение**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A1-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Проверим, что в матрице [[3, 2],[4, 1]] элемент [0,0] = 3\n",
        "# действительно максимален в 0-й строке и минимален в 0-м столбце.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "A = np.array([[3, 2],\n",
        "              [4, 1]])\n",
        "\n",
        "row_max = A.max(axis=1)\n",
        "col_min = A.min(axis=0)\n",
        "\n",
        "print(\"row_max:\", row_max)\n",
        "print(\"col_min:\", col_min)\n",
        "print(\"\\nЭлемент A[0,0] =\", A[0,0])\n",
        "print(\"Максимум в строке 0:\", row_max[0])\n",
        "print(\"Минимум в столбце 0:\", col_min[0])\n",
        "\n",
        "if A[0,0] == row_max[0] and A[0,0] == col_min[0]:\n",
        "    print(\"\\n=> Седловая точка в позиции (0,0)\")\n",
        "else:\n",
        "    print(\"\\n=> Нет седловой точки в (0,0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A2-solution",
      "metadata": {},
      "source": [
        "**(A2) Решение**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A2-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Попробуем нарушить условие седловой точки, например, сделаем элемент [0,1] = 4:\n",
        "# Тогда матрица становится [[3,4],[4,1]], и проверим.\n",
        "\n",
        "A_modified = np.array([[3, 4],\n",
        "                      [4, 1]])\n",
        "\n",
        "row_max_m = A_modified.max(axis=1)\n",
        "col_min_m = A_modified.min(axis=0)\n",
        "print(\"Матрица:\\n\", A_modified)\n",
        "print(\"Максимумы строк:\", row_max_m)\n",
        "print(\"Минимумы столбцов:\", col_min_m)\n",
        "\n",
        "sedlovye = []\n",
        "for i in range(A_modified.shape[0]):\n",
        "    for j in range(A_modified.shape[1]):\n",
        "        if A_modified[i,j] == row_max_m[i] and A_modified[i,j] == col_min_m[j]:\n",
        "            sedlovye.append((i,j))\n",
        "\n",
        "if sedlovye:\n",
        "    print(\"Найдены седловые точки:\", sedlovye)\n",
        "else:\n",
        "    print(\"\\nСедловая точка не найдена.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A3-solution",
      "metadata": {},
      "source": [
        "**(A3) Решение**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A3-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример 2: [[1,3],[2,0]]\n",
        "# Первый игрок играет первую строку (p=[1,0]),\n",
        "# Второй игрок - первый столбец (q=[1,0])\n",
        "\n",
        "A_2 = np.array([[1,3],\n",
        "                [2,0]])\n",
        "P = [1, 0]  # 100% первая строка\n",
        "Q = [1, 0]  # 100% первый столбец\n",
        "\n",
        "print(\"Ожидаемый выигрыш:\", expected_payoff(A_2, P, Q))\n",
        "# Должно быть 1, т.к. A[0,0] = 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1-solution",
      "metadata": {},
      "source": [
        "---\n",
        "### 6.2. Уровень «Средний»\n",
        "\n",
        "**(B1)** В Примере 2 (`[[1,3],[2,0]]`) найдите вручную (на бумаге) оптимальную смешанную стратегию первого игрока.\n",
        "\n",
        "**(B2)** Измените код для игры \"Камень-ножницы-бумага\" (3×3), чтобы второй игрок играл 70% камень, 30% ножницы (0% бумага). Каков будет средний выигрыш первого?\n",
        "\n",
        "**(B3)** Вспомните ещё одну игру 2×2 (например, про \"высокую\" или \"низкую\" цену). Составьте матрицу $2\\times2$. Определите через `nashpy`, есть ли седловая точка.\n",
        "\n",
        "#### Решения (Средний)\n",
        "\n",
        "**(B1) Решение**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B1-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Покажем программно, что p=0.5 - оптимум для первого, q=0.75 - для второго\n",
        "# хотя теоретически выводим уравнением безразличия, здесь проверим 'support_enumeration()'\n",
        "\n",
        "A_b1 = np.array([[1,3],\n",
        "                 [2,0]])\n",
        "game_b1 = nash.Game(A_b1)\n",
        "\n",
        "equilibria_b1 = list(game_b1.support_enumeration())\n",
        "print(\"Равновесия Нэша (p*, q*):\")\n",
        "for eq in equilibria_b1:\n",
        "    print(eq)\n",
        "\n",
        "# Проверим результат:\n",
        "p_star_b1, q_star_b1 = equilibria_b1[0]\n",
        "val_b1 = expected_payoff(A_b1, p_star_b1, q_star_b1)\n",
        "print(\"\\nОптимальная стратегия первого:\", p_star_b1)\n",
        "print(\"Оптимальная стратегия второго:\", q_star_b1)\n",
        "print(\"Значение игры:\", val_b1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B2-solution",
      "metadata": {},
      "source": [
        "**(B2) Решение**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B2-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Модифицируем стратегию второго игрока в RPS:\n",
        "# 70% камень, 30% ножницы, 0% бумага => (0.7, 0.0, 0.3)\n",
        "# Первый играет (1/3, 1/3, 1/3)\n",
        "\n",
        "A_rps_mod = np.array([\n",
        "    [0, -1,  1],\n",
        "    [1,  0, -1],\n",
        "    [-1, 1,  0]\n",
        "])\n",
        "\n",
        "strategy_1_mod = [1/3, 1/3, 1/3]\n",
        "strategy_2_mod = [0.7, 0.0, 0.3]\n",
        "\n",
        "import random\n",
        "\n",
        "def single_round_payoff_rps_mod(A, p, q):\n",
        "    i = np.random.choice(len(p), p=p)\n",
        "    j = np.random.choice(len(q), p=q)\n",
        "    return A[i, j]\n",
        "\n",
        "N_mod = 200000\n",
        "total_mod = 0\n",
        "for _ in range(N_mod):\n",
        "    total_mod += single_round_payoff_rps_mod(A_rps_mod, strategy_1_mod, strategy_2_mod)\n",
        "\n",
        "avg_mod = total_mod / N_mod\n",
        "print(\"Средний выигрыш первого игрока при (1/3,1/3,1/3) vs (0.7,0.0,0.3):\", avg_mod)\n",
        "print(\"(Увеличьте N_mod, чтобы уменьшить флуктуации)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B3-solution",
      "metadata": {},
      "source": [
        "**(B3) Решение**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B3-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Допустим, две фирмы: High/Low. Матрица, например:\n",
        "A_firms = np.array([\n",
        "    [2, 2],  # если 1-я фирма выбрала High, 2-я фирма High/Low\n",
        "    [3, 1]   # если 1-я фирма выбрала Low\n",
        "])\n",
        "\n",
        "game_firms = nash.Game(A_firms)\n",
        "eqs_firms = list(game_firms.support_enumeration())\n",
        "print(\"Платежная матрица:\\n\", A_firms)\n",
        "if eqs_firms:\n",
        "    print(\"\\nНайдены равновесия:\")\n",
        "    for eq in eqs_firms:\n",
        "        print(eq)\n",
        "else:\n",
        "    print(\"\\nНикакого равновесия в чистых или смешанных стратегиях не найдено.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1-solution",
      "metadata": {},
      "source": [
        "---\n",
        "### 6.3. Уровень «Сложный»\n",
        "\n",
        "**(C1)** Составьте **3×3** игру (с нулевой суммой), где нет равновесия в чистых стратегиях. Найдите равновесие в смешанных стратегиях через `nashpy`. Сравните ожидаемый выигрыш, если один из игроков отклонится.\n",
        "\n",
        "**(C2)** Напишите (или допишите) небольшой цикл \"обучения\" в симуляции, где оба игрока корректируют свои вероятности шаг за шагом.\n",
        "\n",
        "**(C3)** Примените подход **линейного программирования** для 3×3 или используйте `scipy.optimize.linprog` для $p_1, p_2, p_3$. Сравните результаты с `support_enumeration()`.\n",
        "\n",
        "#### Решения (Сложный)\n",
        "\n",
        "**(C1) Решение**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C1-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Пример 3x3 игры с нулевой суммой, без равновесия в чистых стратегиях\n",
        "import numpy as np\n",
        "import nashpy as nash\n",
        "\n",
        "A_3x3 = np.array([\n",
        "    [0,   2, -1],\n",
        "    [-3,  1,  2],\n",
        "    [1,   0, -2]\n",
        "])\n",
        "\n",
        "game_3x3 = nash.Game(A_3x3)\n",
        "equilibria_3x3 = list(game_3x3.support_enumeration())\n",
        "print(\"Найдены равновесия в смешанных стратегиях (если есть):\")\n",
        "for eq in equilibria_3x3:\n",
        "    print(eq)\n",
        "\n",
        "if not equilibria_3x3:\n",
        "    print(\"\\nПохоже, не найдено ни одного равновесия. Возможно, алгоритм support_enumeration()\\n\"\n",
        "          \"не обнаружил в чистых, а в смешанных может быть экзотическое решение.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C2-solution",
      "metadata": {},
      "source": [
        "**(C2) Решение**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C2-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Скетч цикла \"обучения\" (упрощённая версия)\n",
        "import random\n",
        "\n",
        "A_learning = np.array([\n",
        "    [0, -1, 1],\n",
        "    [1,  0, -1],\n",
        "    [-1, 1,  0]\n",
        "])  # rps\n",
        "\n",
        "p1 = [1/3, 1/3, 1/3]  # стратегия первого\n",
        "p2 = [0.9, 0.05, 0.05] # стратегия второго\n",
        "\n",
        "def single_round_learning(A, p1, p2):\n",
        "    i = np.random.choice(3, p=p1)\n",
        "    j = np.random.choice(3, p=p2)\n",
        "    payoff = A[i,j]\n",
        "    return i, j, payoff\n",
        "\n",
        "# Наивный цикл: если payoff>0, увеличиваем вероятность выбранного i; если payoff<0, уменьшаем и т.д.\n",
        "# Это лишь иллюстрация, полноценный алгоритм обучения должен быть аккуратнее.\n",
        "\n",
        "learning_steps = 10000\n",
        "alpha = 0.01  # темп обучения\n",
        "\n",
        "for step in range(learning_steps):\n",
        "    i, j, payoff = single_round_learning(A_learning, p1, p2)\n",
        "    # Обновим p1:\n",
        "    if payoff > 0:\n",
        "        p1[i] += alpha\n",
        "    else:\n",
        "        p1[i] -= alpha/2\n",
        "    # Аналог для второго игрока (он проиграл, payoff>0 => второму плохо):\n",
        "    if payoff < 0:\n",
        "        p2[j] += alpha\n",
        "    else:\n",
        "        p2[j] -= alpha/2\n",
        "\n",
        "    # Проекции на simplex:\n",
        "    p1 = [max(0, x) for x in p1]\n",
        "    s1 = sum(p1)\n",
        "    if s1>0:\n",
        "        p1 = [x/s1 for x in p1]\n",
        "\n",
        "    p2 = [max(0, x) for x in p2]\n",
        "    s2 = sum(p2)\n",
        "    if s2>0:\n",
        "        p2 = [x/s2 for x in p2]\n",
        "\n",
        "print(\"После обучения:\")\n",
        "print(\"p1 (стратегия первого):\", p1)\n",
        "print(\"p2 (стратегия второго):\", p2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C3-solution",
      "metadata": {},
      "source": [
        "**(C3) Решение**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C3-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "# LP-подход для 3x3, аналогично 2x2, но с тремя p_i и:\n",
        "# sum_i p_i = 1, p_i >= 0, sum_i(p_i*A[i,j]) >= v для j=0..2\n",
        "# Здесь для примера возьмём ту же A_3x3\n",
        "\n",
        "from scipy.optimize import linprog\n",
        "\n",
        "A_3x3_example = np.array([\n",
        "    [ 0,  2, -1],\n",
        "    [-3,  1,  2],\n",
        "    [ 1,  0, -2]\n",
        "])\n",
        "m, n = A_3x3_example.shape  # должно быть 3,3\n",
        "\n",
        "c = np.zeros(m + 1)\n",
        "c[-1] = -1  # min -v => max v\n",
        "\n",
        "# Ограничение: p1 + p2 + p3 = 1\n",
        "A_eq_3x3 = np.ones((1, m+1))\n",
        "A_eq_3x3[0, -1] = 0\n",
        "b_eq_3x3 = np.array([1])\n",
        "\n",
        "# Неравенства: sum_i p_i * A[i,j] >= v, => -sum_i(p_i*A[i,j]) + v <= 0\n",
        "A_ineq_3x3 = []\n",
        "b_ineq_3x3 = []\n",
        "\n",
        "for j in range(n):\n",
        "    row = np.zeros(m+1)\n",
        "    for i in range(m):\n",
        "        row[i] = -A_3x3_example[i, j]\n",
        "    row[-1] = 1\n",
        "    A_ineq_3x3.append(row)\n",
        "    b_ineq_3x3.append(0)\n",
        "\n",
        "A_ineq_3x3 = np.array(A_ineq_3x3)\n",
        "b_ineq_3x3 = np.array(b_ineq_3x3)\n",
        "\n",
        "# p_i >= 0, v неограничен сверху\n",
        "bounds_3x3 = [(0, None)]*m + [(None, None)]\n",
        "\n",
        "res_3x3 = linprog(c, A_ineq_3x3, b_ineq_3x3, A_eq_3x3, b_eq_3x3,\n",
        "                  bounds=bounds_3x3, method='highs')\n",
        "\n",
        "if res_3x3.success:\n",
        "    p_3x3 = res_3x3.x[:-1]\n",
        "    v_3x3 = res_3x3.x[-1]\n",
        "    # Нормируем p_3x3\n",
        "    if sum(p_3x3)>0:\n",
        "        p_3x3 /= sum(p_3x3)\n",
        "\n",
        "    print(\"Оптимальная стратегия первого игрока:\", p_3x3)\n",
        "    print(\"Значение игры v:\", v_3x3)\n",
        "else:\n",
        "    print(\"LP для 3x3 не нашёл решения.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "mimetype": "text/x-python",
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
